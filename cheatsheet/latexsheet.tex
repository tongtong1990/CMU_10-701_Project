\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage{commath}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[landscape]{geometry}
\newcommand{\vect}[1]{\boldsymbol{#1}}

% To make this come out properly in landscape mode, do one of the following
% 1.
%  pdflatex latexsheet.tex
%
% 2.
%  latex latexsheet.tex
%  dvips -P pdf  -t landscape latexsheet.dvi
%  ps2pdf latexsheet.ps


% If you're reading this, be prepared for confusion.  Making this was
% a learning experience for me, and it shows.  Much of the placement
% was hacked in; if you make it better, let me know...


% 2008-04
% Changed page margin code to use the geometry package. Also added code for
% conditional page margins, depending on paper size. Thanks to Uwe Ziegenhagen
% for the suggestions.

% 2006-08
% Made changes based on suggestions from Gene Cooperman. <gene at ccs.neu.edu>


% To Do:
% \listoffigures \listoftables
% \setcounter{secnumdepth}{0}


% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}

% Turn off header and footer
\pagestyle{empty}
 

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

%\newcommand{\vect}[1]{\boldsymbol{#1}}
% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\textbf{10-701\ Cheat Sheet}} \\
\end{center}

\section{Distributions}
\textbf{Gaussian}: $\ln\frac{1}{(2\pi)^{D/2}} \frac{1}{|\boldsymbol{\Sigma}|^{1/2}} \exp\left\{-\frac{1}{2}(\vect{x} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\vect{x} - \boldsymbol{\mu}) \right\}$; \textbf{multinomial}: $p(\vect{x}|\vect{\mu}) = \prod \mu_k^{x_k}$, where only one of $x_i$ is 1, and others are 0; \textbf{binary}: $\mathrm{Bern}(x|\mu) = \mu^x (1-\mu)^{1-x}$; \textbf{binomial}: $\mathrm{Bin}(m|N,\mu) = \binom{N}{m} \mu^m(1-\mu)^{N-m}$, with expectation $N\mu$ and variance $\mu(1-\mu)$. (reduced to binary for $N=1$) 

\section{Non-Parametric}
MaxLikelihood learning window will give you delta functions, which is a kind of over fitting. Use Leave-one-out cross validation for model selection. Idea: Use some of the data to estimate density; Use other part to evaluate how well it works. Pick the parameter that works bestâ€¨.
$\log p(x_i|X\backslash\cbr{x_i}) = \log \frac{1}{n-1} \sum_{j \neq i} k(x_i, x_j)$, the sum over all points is $\frac{1}{n} \sum_{i=1}^n \log \sbr{\frac{n}{n-1} p(x_i) - \frac{1}{n-1} k(x_i, x_i)} \text{ where } p(x) = \frac{1}{n} \sum_{i=1}^n k(x_i, x)$.

\textbf{why must we not check too many parameters?} that you can overfit more; for a given dataset, a few particular parameter values might happen to do well in k-fold CV by sheer chance, where if you had a new dataset they might not do so well. Checking a reasonable number of parameter values makes you less likely to hit those ``lucky'' spots helps mitigate this risk.

\textbf{Silverman's Rule for kernel size} Use average distance from k nearest neighbors $r_i = \frac{r}{k} \sum_{x \in \mathrm{NN}(x_i, k)} \|x_i - x\|$.

\textbf{Watson Nadaraya} 1. estimate $p(x|y=1) \text{ and } p(x|y=-1)$; 2. compute by Bayes rule $p(y|x) = \frac{p(x|y) p(y)}{p(x)} =  \frac{\frac{1}{m_y} \sum_{y_i = y} k(x_i, x) \cdot \frac{m_y}{m}}  {\frac{1}{m} \sum_i k(x_i, x)}$. 3. Decision boundary $p(y=1|x) - p(y=-1|x) = 
  \frac{\sum_j y_j k(x_j, x)}
  {\sum_i k(x_i, x)} =
\sum_j y_j \frac{k(x_j,x)}{\sum_i k(x_i, x)}$ Actually, we assume that p(x|y) is equal to $1/m_y * \sum_y k(x_i,x)$. Using this definition, we can see $p(x,-1) + p(x,1) = p(x|-1)p(-1)+p(x|1)p(1) = p(x)$.

This can be incorporated into the regression framework in chap 6 of PRML. Where we define $f(x-x_n, t\neq t_n) = 0$, and $f(x-x_n, t=t_n) = f(x-x_n)$. Using this definition, we can derive all the probabilities on this slide. (see my handwritten notes on chap 6 of PRML).

Regression case is the same equation.

\textbf{kNN} Let optimal error rate be $p$. Given unlimited \textbf{iid} data, 1NN's error rate is $\leq 2p(1-p)$.


\section{Matrix Cookbook}
$\frac{\partial \vect{x}^T \vect{a} }{\partial \vect{x}} = \frac{\partial \vect{a}^T \vect{x} }{\partial \vect{x}} = \vect{a}$

$\frac{\partial \vect{a}^T \vect{X} \vect{b}  }{\partial \vect{X}} = \vect{a}\vect{b}^T$, $\frac{\partial \vect{a}^T \vect{X}^T \vect{b}  }{\partial \vect{X}} = \vect{b}\vect{a}^T$, $\frac{\partial \vect{a}^T (\vect{X}^T|\vect{X}) \vect{a}  }{\partial \vect{X}} = \vect{a}\vect{a}^T$

$W \in \vect{S} $, 
$\frac{\partial}{\partial \vect{s} } (\vect{x} - \vect{A}\vect{s})^T \vect{W} (\vect{x} - \vect{A}\vect{s}) = -2\vect{A}^T \vect{W} (\vect{x} - \vect{A}\vect{s})$, 
$\frac{\partial}{\partial \vect{x} } (\vect{x} - \vect{s})^T \vect{W} (\vect{x} - \vect{s}) = 2 \vect{W} (\vect{x} - \vect{s})$, 
$\frac{\partial}{\partial \vect{s} } (\vect{x} - \vect{s})^T \vect{W} (\vect{x} - \vect{s}) = -2 \vect{W} (\vect{x} - \vect{s})$, 
$\frac{\partial}{\partial \vect{x} } (\vect{x} - \vect{A}\vect{s})^T \vect{W} (\vect{x} - \vect{A}\vect{s}) = 2\vect{W} (\vect{x} - \vect{A}\vect{s})$, 
$\frac{\partial}{\partial \vect{A} } (\vect{x} - \vect{A}\vect{s})^T \vect{W} (\vect{x} - \vect{A}\vect{s}) = -2 \vect{W} (\vect{x} - \vect{A}\vect{s})\vect{s}^T$.

$\mathrm{Tr}(\vect{A}) = \sum_i \vect{A}_{ii}$. For two equal sized matrices, $\mathrm{Tr}(\vect{A}^T \vect{B}) = \mathrm{Tr}(\vect{B}^T \vect{A}) = \mathrm{Tr}(\vect{A} \vect{B}^T) = \mathrm{Tr}(\vect{B} \vect{A}^T) = \sum_{i,j} \vect{A}_{ij} \vect{B}_{ij}$. $\mathrm{Tr}(\vect{A}) = \mathrm{Tr}(\vect{A}^T), \mathrm{Tr}(\vect{A}+\vect{B})=\mathrm{Tr}(\vect{A})+\mathrm{Tr}(\vect{B})$. For square matricis, $\mathrm{Tr}(\vect{A}\vect{B})=\mathrm{Tr}(\vect{B}\vect{A}), \mathrm{Tr}(\vect{A}\vect{B}\vect{C})=\mathrm{Tr}(\vect{C}\vect{A}\vect{B})=\mathrm{Tr}(\vect{B}\vect{C}\vect{A})$ (trace rotation).




\section{Classifers and Regressors}
\textbf{Naive Bayes} Conditionally independent: $P(x_1,x_2,\ldots|C) = \prod_i P(x_i|C)$. One way to avoid divide by zero: add $(1,1,\ldots,1)$ and $(0,0,\ldots,0)$ to both classes. 

\emph{Learns} $P(x_i | y)$ for \emph{Discrete $x_i$} -- $P(x_i | y) = \frac{\#D(X_i = x_i, Y = y)}{\#D(Y = y)}$ 
For smoothing, use $P(x_i | y) = \frac{\#D(X_i = x_i, Y = y) + k}{\#D(Y = y) + n_i k}$, where $n_i$ is the number of different possible values for $X_i$ (In practice problem set, Jing Xiang used $k=1$?)
\emph{Continuous $x_i$} -- Can use any PDF, but usually use Gaussian $P(x_i | y) = \mathcal{N}(\mu_{X_i | y}, \sigma_{X_i | y}^2)$, where $\mu_{X_i | y}$ and $\sigma_{X_i | y}$ are, respectively, the average and variance of $X_i$ for all data points where $Y = y$. The Gaussian distribution already provides smoothing.

\textbf{Perceptron} Produces linear decision boundaries. \emph{Classifies} using $\hat{y} = X_{test}\:w + b$ \emph{Learns} $w$ and $b$ by updating $w$ whenever $y_i (w^T x_i + b) \leq 0$ (i.e. incorrectly classified). Updates as  $w \leftarrow w + x_i y_i, b \leftarrow b + y_i$ Repeat until all examples are correctly classified. $w$ is some linear combination $\sum_{i} \alpha_i x_i (y_i * x_i) $ of data points, and decision boundary is the linear hyperplane $f(x) = w^T x + b$. \textbf{Note} that the perceptron is the same as stochastic gradient descent with a hinge loss function of $max(0, 1 - y_i [<w, x_i> + b])$ (\textbf{we can't remove 1 in the loss function; otherwise we can set $w,b=0$}).

\textbf{Convergence of perceptron proof 1} Here we use a perceptron without $b$. Assume we have $\vect{w}^*$ that has margin $\gamma$ ($\min (\vect{w}^*)^T y_i x_i  = \gamma $ ), and $\|\vect{w}^*\| = 1, \|x_i\| = 1$. We start from $\vect{w}_0 = 0$. Assume that we have made $M$ mistakes. We have 1) $\vect{w}_M \cdot \vect{w}^*  = (\vect{w}_{M-1} + y_i x_i)\cdot \vect{w}^* \geq \vect{w}_{M-1} \cdot \vect{w}^* + \gamma$. So we have $\vect{w}_M \cdot \vect{w}^* \geq M \gamma$. 

2) $\vect{w}_M \cdot \vect{w}_M = (\vect{w}_{M-1} + y_i x_i)\cdot (\vect{w}_{M-1} + y_i x_i) = \vect{w}_{M-1} \cdot \vect{w}_{M-1} + 2 y_i x_i \cdot \vect{w}_{M-1} + (y_i x_i) \cdot (y_i x_i) \leq  \vect{w}_{M-1} \cdot \vect{w}_{M-1} + 1$. So we have $\vect{w}_M \cdot \vect{w}_M < M$. 

Combining them, using Cauchy-Schwarz, we have $M\gamma \leq \vect{w}_M \cdot \vect{w}^* \leq \|\vect{w}_M\|  \|\vect{w}^*\| \leq \sqrt{M}$. So $M \leq 1/\gamma^2$.
\textbf{proof 2} Let potential function $Q_i = \|\vect{w}_i\| - \vect{w}_i \cdot \vect{w}^*$, where $i$ is the number of iterations. Assuming up to iteration $i$, we have $M$ mistakes, so we have $Q_i \leq \sqrt{M} - M\gamma$. Clearly $Q_i \geq 0$ by Cauchy-Schwarz. So we have $\sqrt{M} - M\gamma \geq 0$.

\textbf{Linear Regression} 
For $y = \vect{\beta}^T \vect{x}$, $\vect{\beta}^* = (X^T X)^{-1} X^T \vect{y}$, where $X \in \mathbb{R}^{n\times d}$. If we add a regularizing term $\lambda \|\vect{\beta}\|^2$, $\vect{\beta}^* = (X^T X + \lambda I)^{-1} X^T \vect{y}$. Kernalized version of ridge regression: $\vect{\alpha}^* = (X X^T + \lambda I )^{-1}y, \vect{\beta}^* = X^T \vect{\alpha}^*$.

\section{Kernel}
Kernel function $k(\vect{x},\vect{x}') = \phi(\vect{x})^T \phi(\vect{x}')$ for some $\phi(\cdot)$. For a set of data points $\{\vect{x}_i\} $, we have Gram matrix (kernel matrix) $K_{ij} = k(\vect{x}_i, \vect{x}_j)$. A \textbf{necessary and sufficient} condition for being a valid kernel function: $K$ always positive semidefinite. Proof: $\vect{\alpha}^T K \vect{\alpha} = \sum_{ij} \alpha_i \alpha_j K_{ij} = \sum_{ij} \alpha_i \alpha_j \langle \phi(\vect{x}_i), \phi(\vect{x}_j) \rangle = \langle \sum_i \alpha_i \phi(\vect{x}_i),  \sum_j \alpha_j \phi(\vect{x}_j) \rangle \geq 0$.

\textbf{Mercer's Theorem} for any symmtric function $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ which is square integrable and satisfying $\int_{\mathcal{X} \times \mathcal{X}} k(x,x') f(x) f(x') \mathrm{d}x \mathrm{d}x' \geq 0$ for $f \in L_2(\mathcal{X})$, we have a feature space $\Phi(x)$ and $\vect{\lambda} \geq 0 $ that $k(x,x') = \sum_i \lambda_i \phi_i(x) \phi_i(x')$.

\textbf{new kernel from old ones} Given kernels $k_1(x,x'), k_2(x,x')$,  $ck_1(x,x'), f(x)k_1(x,x')f(x'), k_1(x,x')+k_2(x,x'), k_1(x,x')k_2(x,x')$ are new valid kernels. Proof: 1) write the kernel as the dot product of two vectors; 2) use Mercer's Theorem; 3) any Gram matrix derived from it is positive semidefinite. $k_1(x,x')-k_2(x,x')$ is \textbf{invalid}: let $k_1(x,x')= 1$ for $x=x'$, and $0$ otherwise, and $k_2(x,x') = 2k_1(x,x')$. The Gram matrix of new kernel is not PSD. 

\textbf{PSD matrices} \textbf{Products of two PSD matrices are not always PSD}. Let $A$ be $2\times2$ PSD, and $B$ be $\mathrm{diag}(1,2)$. $AB$ is not PSD (columns scaled differently). \textbf{PSD's eigen decomposition} $A=UDU^T$. $A^{m+1} = AA^m = UDU^T(UD^m U^T) = UD(U^T U)D^m U^T = U D^{m+1} U^T$. \textbf{Any PSD is a covariance matrix} Let $\vect{x}$ be a random vector with covariance $I$, PSD $Q$ is the covariance matrix for $Q^{1/2} \vect{x}$: $\mathrm{cov}(Q^{1/2} \vect{x}) = Q^{1/2} \mathrm{cov}(\vect{x}) Q^{1/2} = Q^{1/2}Q^{1/2} = Q$.

\textbf{examples} \textbf{polynomial}: $(\langle x, x' \rangle + c)^d, c\geq 0$. For $c=0$, it's a polynomial having all terms of order $d$; for $c>0$, it contains all terms of order up to $d$. \textbf{gaussian rbf} $\exp(-\lambda\|x-x'\|^2)$. \textbf{laplacian rbf} $\exp(-\lambda|x-x'|^2)$.

\section{Convexity}
\textbf{Convex Sets} \\
\emph{Definition:} A set $C$ is \emph{convex} if the line segment between any two points in $C$ lies in $C$, i.e.\, if for any $x_1, x_2 \in C$ and any $\theta$ with $0 \leq \theta \leq 1$, we have \\
$$ \theta x_1 + (1 - \theta) x_2 \in C $$

\emph{Examples:}
\begin{itemize}
	\item Empty set $\emptyset$, single point ${x_0}$, the whole space $\mathbb{R}^n$
	\item Hyperplane $\{ x | a^T x = b \}$, halfspaces $\{ x | a^T x \leq b \}$
	\item Euclidean balls $\{ x | \| x - x_c \|_2 \leq r \}$
	\item Positive semidefinite marices $S_+^n = \{ A \in S^n | A \succeq 0 \}$ ($S^n$ is the set of symmetric $n \times n$ matrices)
\end{itemize}

\emph{Convexit preserving set operations:}
\begin{itemize}
	\item Translation $\{ x + b | x \in C \}$
	\item Scaling $\{ \lambda x | x \in C \}$
	\item Affine function $\{ Ax + b | x \in C\}$
	\item Intersection $C \cap D$
	\item Set sum $C + D = \{ x + y | x \in C, y \in D \}$
\end{itemize}

\textbf{Convex Functions} \\
\emph{Definition:} A function $f: \mathbb{R}^n \to \mathbb{R}$ is \emph{convex} if $\mathbf{dom} f$ is a convex set and if for all $x$, $y \in \mathbf{dom} f$, and $\theta$ with $0 \leq \theta \leq 1$, we have \\
$$ f(\theta x + (1 - \theta) y) \leq \theta f(x) + (1 - \theta) f(y) $$

\emph{First-order conditions:} Suppose $f$ is differentiable. Then $f$ is convex if and only if $\mathbf{dom} f$ is convex and
$$ f(y) \geq f(x) + \bigtriangledown f(x)^T (y-x) $$

\emph{Second-order conditions:} Assume that $f$ is twice differentiable. Then $f$ is convex if and only if $\mathbf{dom} f$ is convex and its Hessian is positive semidefinite: for all $x \in \mathbf{dom} f$,
$$ {\bigtriangledown}^2 f(x) \succeq 0 $$

\emph{Strict convexity:} Whenever $x \neq y$ and $0 < \theta < 1$, we have
$$ f(\theta x + (1 - \theta) y) < \theta f(x) + (1 - \theta) f(y) $$
Or
$$ f(y) > f(x) + \bigtriangledown f(x)^T (y-x) $$
Or \textbf{sufficient but not necessary condition:}
$$ {\bigtriangledown}^2 f(x) \succ 0 $$

\emph{Strong convexity:} There exists an $m > 0$ such that
$$ f(y) \geq f(x) + \bigtriangledown f(x)^T (y-x) + \frac{m}{2} \| y-x \|_2^2 $$
Or
$$ \bigtriangledown^2 f(x) \succeq mI $$

\emph{Convex function examples:}
\begin{itemize}
	\item Exponential. $e^{ax}$ is convex on $\mathbb{R}$, for any $a \in \mathbb{R}$
	\item Powers. $x^a$ is convex on $\mathbb{R}_{++}$ when $a \geq 1$ or $a \leq 0$, and concave for $0 \leq a \leq 1$
	\item Powers of absolute value. $|x|^p$ for $p \geq 1$ is convex on $\mathbb{R}$
	\item Logatithm. $\log x$ is concave on $\mathbb{R}_++$
	\item Norms. Every norm on $\mathbb{R}^n$ is convex
	\item $f(x) = \max{x_1, ... , x_n}$ is convex on $\mathbb{R}^n$
	\item Log-sum-exp. $f(x) = \log(e^{x_1} + ... + e^{x_n})$ is convex on $\mathbb{R}^n$
\end{itemize}

\emph{Convexity preserving function operations}
Convex functions $f(x), g(x)$
\begin{itemize}
	\item Nonnegative weighted sum: $a f(x) + b g(x)$
	\item Pointwise maximum: $f(x) = \max{f_1(x), ... , f_m(x)}$
	\item Composition with affine function: $f(Ax+b)$
	\item Composition with nondecreasin convex function $g$: $g(f(x))$
\end{itemize}





\section{Duality}
\textbf{primal problem} (standard form): $\min f_0(x), \text{s.t.} f_i(x) \leq 0, i = 1,\ldots,m, h_i(x)=0, i = 1,\ldots,p$.
\textbf{Lagrangian}: $\mathbb{R}^n \times \mathbb{R}^m \times \mathbb{R}^p \rightarrow R$, $L(x,\lambda,\nu) = f_0(x) + \sum_i \lambda_i f_i (x) + \sum_i \nu_i h_i(x)$.
\textbf{Lagrange dual function} $\mathbb{R}^m \times \mathbb{R}^p \rightarrow R$, $g(\lambda, \nu) = \inf_{x \in \mathcal{D}} L(x, \lambda, \nu)$. $g$ is concave (can be $-\infty$ for some $\lambda,\nu$). \textbf{lower bound property} if $\lambda \succeq 0$, $g(\lambda, \nu) \leq p^*$.
\textbf{Lagrange dual problem} $\max g(\lambda,\nu), \text{s.t.} \lambda \succeq 0$. \textbf{KKT conditions} For $(x^*, \lambda^*, \nu^*)$, we have 1) $f_i(x^*) \leq 0$, 2) $h_i(x^*) = 0$, 3) $\lambda^*_i \geq 0$, 4) $\lambda^*_i f_i(x^*) = 0$, 5) $\nabla_x L(x,\lambda,\nu) = 0$. Or 1) primal constraints; 2) dual constraints; 3) complementary slackness; 4) gradient of Lagrangian with respect to $x$ is zero. If strong duality holds, then optimal $(x^*, \lambda^*, \nu^*)$ must satisfy KKT. For a convex problem, finding $(x^*, \lambda^*, \nu^*)$ satifsying KKT means they're optimal (and proves the problem has strong duality). If Slater's condition is satisfied, $x$ is optimal iff there exists $(\lambda, \nu)$ that satisfy KKT.

\textbf{dual for LP} for standard LP $\min c^T x, \text{s.t.} Ax=b, x \succeq 0$, we have dual $\max -b^T \nu, \text{s.t.} A^T\nu - \lambda + c = 0, \lambda \succeq 0$. for inequality LP $\min c^T x, \text{s.t.} Ax\preceq b$, we have the dual problem $\max -b^T \lambda, \text{s.t.} A^T\lambda + c = 0, \lambda \succeq 0$.

\section{SVM}
\textbf{primal form} $\min_{w,b} 1/2 \|w\|^2, \text{s.t.} (w^T x_i + b)y_i \geq 1$.
\textbf{Lagrangian} $L(w,b,\alpha) = 1/2 \|w\|^2 - \sum_i \alpha_i [(w^T x_i + b)y_i - 1]$. Minimize w.r.t. $w,b$, we have $\partial_w L(w,b,\alpha) = w-\sum_i \alpha_i y_i x_i = 0, \partial_b L(w,b,\alpha) =  \sum_i \alpha_i y_i = 0$.
\textbf{dual form} Plugging back, we have $\max -1/2 \sum_{ij} \alpha_i \alpha_j y_i y_j \langle x_i, x_j \rangle + \sum_i \alpha_i$, with constraints $\sum_i \alpha_i y_i = 0$ and $\alpha_i \geq 0$. After solving this, we can preserve the input vectors that have associated $\alpha_i > 0$. These are support vectors.


\rule{0.3\linewidth}{0.25pt}
\scriptsize

Copyright \copyright\ 2013 Yimeng Zhang.

\end{multicols}
\end{document}
