\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage{commath}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[landscape]{geometry}
\newcommand{\vect}[1]{\boldsymbol{#1}}

% To make this come out properly in landscape mode, do one of the following
% 1.
%  pdflatex latexsheet.tex
%
% 2.
%  latex latexsheet.tex
%  dvips -P pdf  -t landscape latexsheet.dvi
%  ps2pdf latexsheet.ps


% If you're reading this, be prepared for confusion.  Making this was
% a learning experience for me, and it shows.  Much of the placement
% was hacked in; if you make it better, let me know...


% 2008-04
% Changed page margin code to use the geometry package. Also added code for
% conditional page margins, depending on paper size. Thanks to Uwe Ziegenhagen
% for the suggestions.

% 2006-08
% Made changes based on suggestions from Gene Cooperman. <gene at ccs.neu.edu>


% To Do:
% \listoffigures \listoftables
% \setcounter{secnumdepth}{0}


% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}

% Turn off header and footer
\pagestyle{empty}
 

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

%\newcommand{\vect}[1]{\boldsymbol{#1}}
% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\textbf{10-701\ Cheat Sheet}} \\
\end{center}

\section{Non-Parametric}
MaxLikelihood learning window will give you delta functions, which is a kind of over fitting. Use Leave-one-out cross validation for model selection. Idea: Use some of the data to estimate density; Use other part to evaluate how well it works. Pick the parameter that works bestâ€¨.
$\log p(x_i|X\backslash\cbr{x_i}) = \log \frac{1}{n-1} \sum_{j \neq i} k(x_i, x_j)$, the sum over all points is $\frac{1}{n} \sum_{i=1}^n \log \sbr{\frac{n}{n-1} p(x_i) - \frac{1}{n-1} k(x_i, x_i)} \text{ where } p(x) = \frac{1}{n} \sum_{i=1}^n k(x_i, x)$.

\textbf{why must we not check too many parameters?} that you can overfit more; for a given dataset, a few particular parameter values might happen to do well in k-fold CV by sheer chance, where if you had a new dataset they might not do so well. Checking a reasonable number of parameter values makes you less likely to hit those ``lucky'' spots helps mitigate this risk.

\textbf{Silverman's Rule for kernel size} Use average distance from k nearest neighbors $r_i = \frac{r}{k} \sum_{x \in \mathrm{NN}(x_i, k)} \|x_i - x\|$.

\textbf{Watson Nadaraya} 1. estimate $p(x|y=1) \text{ and } p(x|y=-1)$; 2. compute by Bayes rule $p(y|x) = \frac{p(x|y) p(y)}{p(x)} =  \frac{\frac{1}{m_y} \sum_{y_i = y} k(x_i, x) \cdot \frac{m_y}{m}}  {\frac{1}{m} \sum_i k(x_i, x)}$. 3. Decision boundary $p(y=1|x) - p(y=-1|x) = 
  \frac{\sum_j y_j k(x_j, x)}
  {\sum_i k(x_i, x)} =
\sum_j y_j \frac{k(x_j,x)}{\sum_i k(x_i, x)}$ Actually, we assume that p(x|y) is equal to $1/m_y * \sum_y k(x_i,x)$. Using this definition, we can see $p(x,-1) + p(x,1) = p(x|-1)p(-1)+p(x|1)p(1) = p(x)$.

This can be incorporated into the regression framework in chap 6 of PRML. Where we define $f(x-x_n, t\neq t_n) = 0$, and $f(x-x_n, t=t_n) = f(x-x_n)$. Using this definition, we can derive all the probabilities on this slide. (see my handwritten notes on chap 6 of PRML).

Regression case is the same equation.

\textbf{kNN} Let optimal error rate be $p$. Given unlimited \textbf{iid} data, 1NN's error rate is $\leq 2p(1-p)$.


\section{Matrix Cookbook}
$\frac{\partial \vect{x}^T \vect{a} }{\partial \vect{x}} = \frac{\partial \vect{a}^T \vect{x} }{\partial \vect{x}} = \vect{a}$

$\frac{\partial \vect{a}^T \vect{X} \vect{b}  }{\partial \vect{X}} = \vect{a}\vect{b}^T$, $\frac{\partial \vect{a}^T \vect{X}^T \vect{b}  }{\partial \vect{X}} = \vect{b}\vect{a}^T$, $\frac{\partial \vect{a}^T (\vect{X}^T|\vect{X}) \vect{a}  }{\partial \vect{X}} = \vect{a}\vect{a}^T$

$W \in \vect{S} $, 
$\frac{\partial}{\partial \vect{s} } (\vect{x} - \vect{A}\vect{s})^T \vect{W} (\vect{x} - \vect{A}\vect{s}) = -2\vect{A}^T \vect{W} (\vect{x} - \vect{A}\vect{s})$, 
$\frac{\partial}{\partial \vect{x} } (\vect{x} - \vect{s})^T \vect{W} (\vect{x} - \vect{s}) = 2 \vect{W} (\vect{x} - \vect{s})$, 
$\frac{\partial}{\partial \vect{s} } (\vect{x} - \vect{s})^T \vect{W} (\vect{x} - \vect{s}) = -2 \vect{W} (\vect{x} - \vect{s})$, 
$\frac{\partial}{\partial \vect{x} } (\vect{x} - \vect{A}\vect{s})^T \vect{W} (\vect{x} - \vect{A}\vect{s}) = 2\vect{W} (\vect{x} - \vect{A}\vect{s})$, 
$\frac{\partial}{\partial \vect{A} } (\vect{x} - \vect{A}\vect{s})^T \vect{W} (\vect{x} - \vect{A}\vect{s}) = -2 \vect{W} (\vect{x} - \vect{A}\vect{s})\vect{s}^T$, 


\section{Classifers and Regressors}
\emph{Naive Bayes} Conditionally independent: $P(x_1,x_2,\ldots|C) = \prod_i P(x_i|C)$. One way to avoid divide by zero: add $(1,1,\ldots,1)$ and $(0,0,\ldots,0)$ to both classes. 

\textbf{Learns} $P(x_i | y)$ for \emph{Discrete $x_i$} -- $P(x_i | y) = \frac{\#D(X_i = x_i, Y = y)}{\#D(Y = y)}$ 
For smoothing, use $P(x_i | y) = \frac{\#D(X_i = x_i, Y = y) + k}{\#D(Y = y) + n_i k}$, where $n_i$ is the number of different possible values for $X_i$ (In practice problem set, Jing Xiang used $k=1$?)
\emph{Continuous $x_i$} -- Can use any PDF, but usually use Gaussian $P(x_i | y) = \mathcal{N}(\mu_{X_i | y}, \sigma_{X_i | y}^2)$, where $\mu_{X_i | y}$ and $\sigma_{X_i | y}$ are, respectively, the average and variance of $X_i$ for all data points where $Y = y$. The Gaussian distribution already provides smoothing.

\emph{Perceptron} Produces linear decision boundaries. \emph{Classifies} using $\hat{y} = X_{test}\:w + b$ \emph{Learns} $w$ and $b$ by updating $w$ whenever $y_i (w^T x_i + b) \leq 0$ (i.e. incorrectly classified). Updates as  $w \leftarrow w + x_i y_i, b \leftarrow b + y_i$ Repeat until all examples are correctly classified. $w$ is some linear combination $\sum_{i} \alpha_i x_i (y_i * x_i) $ of data points, and decision boundary is the linear hyperplane $f(x) = w^T x + b$. \textbf{Note} that the perceptron is the same as stochastic gradient descent with a hinge loss function of $max(0, 1 - y_i [<w, x_i> + b])$ (\textbf{is this loss function right}).

\textbf{Convergence of perceptron proof 1 by holy shit smola}
\textbf{Convergence of perceptron proof 2 by gordon}

\rule{0.3\linewidth}{0.25pt}
\scriptsize

Copyright \copyright\ 2013 Yimeng Zhang.

\end{multicols}
\end{document}
